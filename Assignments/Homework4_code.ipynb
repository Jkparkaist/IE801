{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import nash\n",
    "import random\n",
    "from python_utils import *\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This is grid game 1 introudced in the lecture note\n",
    "#Here, we modify reward so that it can converge Nash equilibrum more faster\n",
    "#That is, we give -1 for every transition except the transition to the goal state\n",
    "#you don't have to modify this game simulator but need to understand how it works\n",
    "\n",
    "#state: [[Agent1's row index, Agent1's column index],[Agent2's row index, Agent2's column index]] \n",
    "#for example, [[2,0],[2,2]] represents\n",
    "#   0    1    2\n",
    "#0[   ][   ][   ]\n",
    "#1[   ][   ][   ]\n",
    "#2[Ag1][   ][Ag2]\n",
    "\n",
    "#goalStates is [[0,2],[0,0]]\n",
    "#   0        1      2\n",
    "#0[Ag2's G][   ][Ag1's G]\n",
    "#1[       ][   ][       ]\n",
    "#2[       ][   ][       ]\n",
    "\n",
    "\n",
    "class gridGame1:\n",
    "    \n",
    "    def __init__(self, height, width):\n",
    "        \n",
    "        self.agentNum = 2\n",
    "        self.GAMMA = 0.99\n",
    "                      \n",
    "        self.WORLD_HEIGHT = height\n",
    "        self.WORLD_WIDTH = width\n",
    "        \n",
    "        self.ACTION_UP = 0\n",
    "        self.ACTION_DOWN = 1\n",
    "        self.ACTION_LEFT = 2\n",
    "        self.ACTION_RIGHT = 3\n",
    "        self.actions = [self.ACTION_UP, self.ACTION_DOWN, self.ACTION_LEFT, self.ACTION_RIGHT]\n",
    "        \n",
    "        self.startState = [[self.WORLD_HEIGHT-1, 0], [self.WORLD_HEIGHT-1, self.WORLD_WIDTH-1]]\n",
    "        self.goalState = [[0, self.WORLD_WIDTH-1], [0, 0]]\n",
    "        self.actionDestination = []\n",
    "        for i in range(0, self.WORLD_HEIGHT):\n",
    "            self.actionDestination.append([])\n",
    "            for j in range(0, self.WORLD_WIDTH):\n",
    "                destinaion = dict()\n",
    "                destinaion[self.ACTION_UP] = [max(i - 1, 0), j]\n",
    "                destinaion[self.ACTION_DOWN] = [min(i + 1, self.WORLD_HEIGHT - 1), j]\n",
    "                destinaion[self.ACTION_LEFT] = [i, max(j - 1, 0)]\n",
    "                destinaion[self.ACTION_RIGHT] = [i, min(j + 1, self.WORLD_WIDTH - 1)]\n",
    "                self.actionDestination[-1].append(destinaion)\n",
    "    \n",
    "    #this function returns (jointNextState, jointRewards) at the same time\n",
    "    def getNextJointState(self, currentJointState, jointActions):\n",
    "        \n",
    "        state0 = currentJointState[0]\n",
    "        state1 = currentJointState[1]\n",
    "        action0 = jointActions[0]\n",
    "        action1 = jointActions[1]\n",
    "        nextState0 = self.actionDestination[state0[0]][state0[1]][action0]\n",
    "        nextState1 = self.actionDestination[state1[0]][state1[1]][action1]\n",
    "        if nextState0 == self.goalState[0] and nextState1 == self.goalState[1]:\n",
    "            return [nextState0, nextState1], [100, 100]\n",
    "        if nextState0 == self.goalState[0] and nextState1 != self.goalState[1]:\n",
    "            return [nextState0, nextState1], [100, 0]\n",
    "        if nextState0 != self.goalState[0] and nextState1 == self.goalState[1]:\n",
    "            return [nextState0, nextState1], [0, 100]\n",
    "        if nextState0 == nextState1 and nextState0 != self.goalState[0]:\n",
    "            return [state0, state1], [-1, -1]\n",
    "        else:\n",
    "            return [nextState0, nextState1], [-1, -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This is grid game 1 introudced in the lecture note\n",
    "#Here, we modify reward so that it can converge Nash equilibrum more faster\n",
    "#That is, we give -1 for every transition except the transition to the goal state\n",
    "#you don't have to modify this game simulator but need to understand how it works\n",
    "\n",
    "#state: [[Agent1's row index, Agent1's column index],[Agent2's row index, Agent2's column index]] \n",
    "#for example, [[2,0],[2,2]] represents\n",
    "#   0    1    2\n",
    "#0[   ][   ][   ]\n",
    "#1[   ][   ][   ]\n",
    "#2[Ag1][   ][Ag2]\n",
    "\n",
    "#goalStates is [[0,1],[0,1]]\n",
    "#   0        1      2\n",
    "#0 [    ][Common Goal][   ]\n",
    "#1 [    ][           ][   ]\n",
    "#2 [    ][           ][   ]\n",
    "\n",
    "class gridGame2:\n",
    "    \n",
    "    def __init__(self, height, width):\n",
    "        \n",
    "        self.agentNum = 2\n",
    "        self.GAMMA = 0.99              \n",
    "        self.WORLD_HEIGHT = height\n",
    "        self.WORLD_WIDTH = width\n",
    "        \n",
    "        self.ACTION_UP = 0\n",
    "        self.ACTION_DOWN = 1\n",
    "        self.ACTION_LEFT = 2\n",
    "        self.ACTION_RIGHT = 3\n",
    "        self.actions = [self.ACTION_UP, self.ACTION_DOWN, self.ACTION_LEFT, self.ACTION_RIGHT]\n",
    "        \n",
    "        self.startState = [[self.WORLD_HEIGHT - 1, 0],[self.WORLD_HEIGHT - 1, self.WORLD_WIDTH - 1]]\n",
    "        self.goalState = [[0, np.floor(self.WORLD_WIDTH/2)],[0, np.floor(self.WORLD_WIDTH/2)]]\n",
    "        \n",
    "        self.actionDestination = []\n",
    "        for i in range(0, self.WORLD_HEIGHT):\n",
    "            self.actionDestination.append([])\n",
    "            for j in range(0, self.WORLD_WIDTH):\n",
    "                destinaion = dict()\n",
    "                destinaion[self.ACTION_UP] = [max(i - 1, 0), j]\n",
    "                destinaion[self.ACTION_LEFT] = [i, max(j - 1, 0)]\n",
    "                destinaion[self.ACTION_RIGHT] = [i, min(j + 1, self.WORLD_WIDTH - 1)]\n",
    "                destinaion[self.ACTION_DOWN] = [min(i + 1, self.WORLD_HEIGHT - 1), j]\n",
    "                self.actionDestination[-1].append(destinaion)\n",
    "        \n",
    "        \n",
    "    def getNextJointState(self, currentJointState, jointActions):\n",
    "        \n",
    "        state0 = currentJointState[0]\n",
    "        state1 = currentJointState[1]\n",
    "        action0 = jointActions[0]\n",
    "        action1 = jointActions[1]\n",
    "        nextState0 = self.actionDestination[state0[0]][state0[1]][action0]\n",
    "        nextState1 = self.actionDestination[state1[0]][state1[1]][action1]\n",
    "        \n",
    "        #when facing semi wall, the transitions are stochastic\n",
    "        if state0 == [self.WORLD_HEIGHT-1, 0] or state0 == [self.WORLD_HEIGHT-1, self.WORLD_WIDTH-1]:\n",
    "            if action0 == self.ACTION_UP and random.random() > 0.5:\n",
    "                nextState0 = state0\n",
    "           \n",
    "        if state1 == [self.WORLD_HEIGHT-1, 0] or state1 == [self.WORLD_HEIGHT-1, self.WORLD_WIDTH-1]:\n",
    "            if action1 == self.ACTION_UP and random.random() > 0.5:\n",
    "                nextState1 = state1\n",
    "        \n",
    "        #if Ag1 or Ag2 reaches goal\n",
    "        if nextState0 == self.goalState[0] and nextState1 == self.goalState[1]:\n",
    "            return [nextState0, nextState1], [100, 100]\n",
    "        elif nextState0 == self.goalState[0] and nextState1 != self.goalState[1]:\n",
    "            return [nextState0, nextState1], [100, 0]\n",
    "        elif nextState0 != self.goalState[0] and nextState1 == self.goalState[1]:\n",
    "            return [nextState0, nextState1], [0, 100]\n",
    "        \n",
    "        #if two agents colide to each other\n",
    "        if nextState0 == nextState1 and nextState1 != self.goalState[1]:\n",
    "            return [state0, state1], [-1, -1]\n",
    "        else:\n",
    "            return [nextState0, nextState1], [-1, -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "#qLearner maintains and update its own Nash Q value and choose best action with respect its own action\n",
    "class qLearner:\n",
    "        \n",
    "    def __init__(self, game, idx):\n",
    "\n",
    "        self.game = game\n",
    "        \n",
    "        #agent identifier (idx=0 for Agent 1, idx=1 for Agent 2 )\n",
    "        self.idx = idx\n",
    "        \n",
    "        #learning rate that will be decreasing with the iteration\n",
    "        self.ALPHA = 1\n",
    "        \n",
    "        #discount factor, it is related with game objective\n",
    "        self.GAMMA = game.GAMMA\n",
    "        \n",
    "        #to predefine the Q tables receive the size of game\n",
    "        self.WORLD_HEIGHT = game.WORLD_HEIGHT\n",
    "        self.WORLD_WIDTH = game.WORLD_WIDTH\n",
    "        \n",
    "        #define actions sets        \n",
    "        self.actions = game.actions\n",
    "        \n",
    "        #Initial Q-values\n",
    "        #Q-value is represented as multidimensional matirx\n",
    "        #Q(agent's row, agent's column, agent's action)\n",
    "        self.Q = np.zeros((self.WORLD_HEIGHT, self.WORLD_WIDTH, len(game.actions)))\n",
    "    \n",
    "        #number of visits that will be used to decrease the learning rate self.ALPHA, i.e., alpha=1/numberOfVisit\n",
    "        self.visitCounter = np.zeros((self.WORLD_HEIGHT, self.WORLD_WIDTH, len(game.actions)))\n",
    "            \n",
    "        \n",
    "    def resetQValue(self):    \n",
    "        self.Q = np.zeros((self.WORLD_HEIGHT, self.WORLD_WIDTH, len(self.game.actions)))\n",
    "        self.visitCounter = np.zeros((self.WORLD_HEIGHT, self.WORLD_WIDTH, len(self.game.actions)))\n",
    "\n",
    "    def chooseAction(self,currentJointState):\n",
    "        #extract the state for the target agent labled with 'self.idx'\n",
    "        currentState = currentJointState[self.idx]\n",
    "        actionSelected = np.argmax(self.Q[currentState[0], currentState[1], :])\n",
    "        return actionSelected\n",
    "    \n",
    "    \n",
    "    #eventhough the agent is provided with jointstate and jointAction, jointReward, it will use its own information\n",
    "    def updateQ(self, currentJointState, jointAction,nextJointState, jointReward):   \n",
    "        currentState = currentJointState[self.idx]        \n",
    "        nextState = nextJointState[self.idx]\n",
    "        action = jointAction[self.idx]\n",
    "        reward = jointReward[self.idx]\n",
    "        \n",
    "        ######################################################################################\n",
    "        ################################ Fill up your own code bellow#########################\n",
    "        ####### the bellow is just for guide that does not necessarily be followed ###########\n",
    "         \n",
    "            \n",
    "        #get he current Q values Q(s,a)\n",
    "        \n",
    "        \n",
    "        #get the number of visit to adjust learning rate\n",
    "               \n",
    "            \n",
    "        #compute the optimal Q-values at the next state: max_a Q(s',a)             \n",
    "        \n",
    "        \n",
    "        #update Q values\n",
    "        \n",
    "        \n",
    "        #update visitCounter\n",
    "        \n",
    "        \n",
    "        #############################################################\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nashQLearner maintains Nash Q values for all the agnets and update them with\n",
    "# (1) currentJointState\n",
    "# (2) jointAction\n",
    "# (3) nextJointState\n",
    "class nashQLearner:\n",
    "    def __init__(self, game, idx):       \n",
    "        self.game = game\n",
    "        \n",
    "        #agent identifier (idx=0 for Agent 1, idx=1 for Agent 2 )\n",
    "        self.idx = idx\n",
    "        \n",
    "        #learning rate that will be decreasing with the iteration\n",
    "        self.ALPHA = 1\n",
    "        \n",
    "        #discount factor, it is related with game objective\n",
    "        self.GAMMA = game.GAMMA\n",
    "        \n",
    "        #to predefine the Q tables receive the size of game\n",
    "        self.WORLD_HEIGHT = game.WORLD_HEIGHT\n",
    "        self.WORLD_WIDTH = game.WORLD_WIDTH\n",
    "    \n",
    "        #Initial Q-values\n",
    "        #Q-value is represented as multidimensional matirx\n",
    "        #Q(agent1's row, agent1's column, agent2's row, agent2's column, agent1's action, agent2's action)\n",
    "        \n",
    "        self.Q1 = np.zeros((self.WORLD_HEIGHT, self.WORLD_WIDTH, self.WORLD_HEIGHT, self.WORLD_WIDTH, len(game.actions), len(game.actions)))\n",
    "        self.Q2 = np.zeros((self.WORLD_HEIGHT, self.WORLD_WIDTH, self.WORLD_HEIGHT, self.WORLD_WIDTH, len(game.actions), len(game.actions)))\n",
    "    \n",
    "        #number of visits that will be used to decrease the learning rate self.ALPHA, i.e., alpha=1/numberOfVisit\n",
    "        self.visitCounter = np.zeros((self.WORLD_HEIGHT, self.WORLD_WIDTH, self.WORLD_HEIGHT, self.WORLD_WIDTH, len(game.actions), len(game.actions)))\n",
    "    \n",
    "    \n",
    "    #compute Nash-Equilibrium Q values, i.e., Nash Q_i(currentJointState) for all agents\n",
    "    #and output Nash equilibrium joint actions\n",
    "    def computeNash(self, currentJointState):\n",
    "        ag1_row = currentJointState[0][0]\n",
    "        ag1_col = currentJointState[0][1]\n",
    "        ag2_row = currentJointState[1][0]\n",
    "        ag2_col = currentJointState[1][1]\n",
    "        \n",
    "        #extract Nash Q values at the current state\n",
    "        Q1_matrix=self.Q1[ag1_row][ag1_col][ag2_row][ag2_col]\n",
    "        Q2_matrix=self.Q2[ag1_row][ag1_col][ag2_row][ag2_col]\n",
    "        payOffMatirx=[Q1_matrix,Q2_matrix]\n",
    "        \n",
    "        \n",
    "        #define stage game and solve equilibrium using nash module (refer nashpy)\n",
    "        stageGame=nash.Game(Q1_matrix,Q2_matrix)\n",
    "        eqList=[]\n",
    "        valueList=[]\n",
    "        for eq in stageGame.equilibria():\n",
    "            \n",
    "            #eq is represented as follows\n",
    "            #eq[0]=[1, 0] #player 1's strategy\n",
    "            #eq[1]=[0.7,0.3] #player 2's strategy\n",
    "            eqList.append(eq)\n",
    "\n",
    "        #if it cannot find a Nash equilibrium, just gives the current Nash Q-values\n",
    "        if len(eqList)==0:\n",
    "            action1 = np.random.randint(4)\n",
    "            action2 = np.random.randint(4)\n",
    "            nashQ1 = Q1_matrix[action1][action2]\n",
    "            nashQ2 = Q2_matrix[action1][action2]\n",
    "        else:  \n",
    "            #choose always first Nash actions\n",
    "            eq=eqList[0]\n",
    "                \n",
    "            #agent 1's chosen action is the action with the largest probablity in agent1's mixed strategy eq[0]\n",
    "            action1=np.argmax(eq[0]) \n",
    "            #Nash equilibrium value computed as u1=s1*U1*s2'\n",
    "            nashQ1=eq[0].dot(Q1_matrix).dot(eq[1])\n",
    "            \n",
    "            #agent 2's chosen action is the action with the largest probablity in agent2's mixed strategy eq[1]\n",
    "            action2=np.argmax(eq[1])\n",
    "            #Nash equilibrium value computed as u1=s1*U2*s2'\n",
    "            nashQ2=eq[0].dot(Q2_matrix).dot(eq[1])\n",
    "            \n",
    "        actions = [action1,action2]\n",
    "        payoffs = [nashQ1, nashQ2]\n",
    "        \n",
    "        return actions, payoffs\n",
    "          \n",
    "    \n",
    "    #when repeat the simulation, we need to reset all Q values\n",
    "    def resetQValue(self):\n",
    "        self.Q1 = np.zeros((self.WORLD_HEIGHT, self.WORLD_WIDTH, self.WORLD_HEIGHT, self.WORLD_WIDTH, len(self.game.actions), len(self.game.actions)))\n",
    "        self.Q2 = np.zeros((self.WORLD_HEIGHT, self.WORLD_WIDTH, self.WORLD_HEIGHT, self.WORLD_WIDTH, len(self.game.actions), len(self.game.actions)))\n",
    "        self.visitConter = np.zeros((self.WORLD_HEIGHT, self.WORLD_WIDTH, self.WORLD_HEIGHT, self.WORLD_WIDTH, len(self.game.actions), len(self.game.actions)))\n",
    "        \n",
    "    #action selection rule: we select the Nash equilibirum action: (a1, a2) = Nash (Q1(s,a1,a2),Q2(s,a1,a2))    \n",
    "    def chooseAction(self,currentJointState):\n",
    "        actions, payoffs = self.computeNash(currentJointState)\n",
    "        \n",
    "        #provide target agnet's action only (idx=0 for Agent 1, idx=1 for Agent2)\n",
    "        actionSelected = actions[self.idx]\n",
    "        \n",
    "        return actionSelected\n",
    "\n",
    "    #Nash Q values for all the agnets and update them with (1)currentJointState, (2)jointAction, and(3) nextJointState  \n",
    "    def updateQ(self, currentJointState, jointAction,nextJointState, jointReward):\n",
    "        \n",
    "        \n",
    "        ######################################################################################\n",
    "        ################################ Fill up your own code bellow#########################\n",
    "        ####### the bellow is just for guide that does not necessarily be followed ###########\n",
    "        \n",
    "        \n",
    "        #index for locating Q values\n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "        #current estimations q1=Q1(currentJointStae,jointAction),q2=Q2(currentJointStae,jointAction)\n",
    "\n",
    "        \n",
    "        #obtain Nash equlibirum values at the nextJointState, \n",
    "        #i.e., NashQ_1(nextJointState) and NashQ_2(nextJointState)\n",
    "\n",
    "        \n",
    "        \n",
    "        #Update Nash Q-values using Nash-Q learning equation\n",
    "        # Q_i(s,a)=Q_i(s,a)+alpha(r1+gamma*Nash Q_i(s')- Q_i(s,a))\n",
    "       \n",
    "       \n",
    "        #update visitCounter\n",
    "        \n",
    "        \n",
    "        ######################################################################################\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "class agentManager:\n",
    "        \n",
    "    #manager will maintain the game's currentJointState, jointAction, nextJointState    \n",
    "        \n",
    "    def __init__(self):    \n",
    "        self.agentSet = []\n",
    "        self.currentJointState = []\n",
    "        self.agentNum = 0\n",
    "    \n",
    "    #Manager can manage agents, we can add each agent.\n",
    "    def addAgent(self, agent):\n",
    "        self.agentSet.append(agent)\n",
    "        self.agentNum += 1\n",
    "    \n",
    "    #aggregate the actions from agents to form a jointAction\n",
    "    def chooseJointAction(self,currentJointState):\n",
    "        jointAction = []\n",
    "        for i in range(self.agentNum):\n",
    "            jointAction.append(self.agentSet[i].chooseAction(currentJointState))\n",
    "        return jointAction\n",
    " \n",
    "    def updateJointState(self,jointState):\n",
    "        self.currentJointState = jointState\n",
    "            \n",
    "    def resetQValues(self):\n",
    "        for i in range(self.agentNum):\n",
    "            self.agentSet[i].resetQValue()\n",
    "    \n",
    "    #manager will provoke each agent to update their Nash Q values by providing (s,a,s')\n",
    "    def updateQValueSet(self,currentJointState,jointAction, nextJointState, jointReward):\n",
    "        for i in range(self.agentNum):        \n",
    "            self.agentSet[i].updateQ(currentJointState, jointAction, nextJointState, jointReward)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ExperiementRunner accept game and manger to proceed the learning proceudre \n",
    "class experimentRunner:\n",
    "    \n",
    "    #the inputs are \n",
    "    # (1) the game that a set of agents are interacting with\n",
    "    # (2) the manger with n agents to interact with game\n",
    "    def __init__(self, game, manager):\n",
    "        self.game = game\n",
    "        self.manager = manager\n",
    "        self.agentNum = manager.agentNum\n",
    "        self.EPSILON = 0.5\n",
    "    \n",
    "    #conduct one episode of the target game and update Q-values for all the agents (learning)\n",
    "    def doEpisodes(self,number=1):\n",
    "        \n",
    "        #every episode start from game startingState\n",
    "        self.manager.updateJointState(self.game.startState)    \n",
    "        \n",
    "        #accumulated will be computed for every episode\n",
    "        accumulatedRewards = []\n",
    "        for n in range(self.agentNum):\n",
    "            accumulatedRewards.append(0.0)\n",
    "        \n",
    "        #count will be used to discount imediate reward\n",
    "        count = 0\n",
    "        \n",
    "        a = True\n",
    "        while a:\n",
    "            \n",
    "            #1: get the currentJointState from Manager\n",
    "            currentJointState = self.manager.currentJointState             \n",
    "            \n",
    "            #2: select action at the given state using exploration strategy\n",
    "            #this is greedy action with Nash equilibrium for exploitation\n",
    "            jointAction = self.manager.chooseJointAction(currentJointState)\n",
    "            #this is random action for exploration\n",
    "            if np.random.binomial(1, self.EPSILON) == 1:\n",
    "                a0 = np.random.choice(self.game.actions)\n",
    "                a1 = np.random.choice(self.game.actions)\n",
    "                jointAction = [a0, a1]\n",
    "        \n",
    "            #3: find the nextJointState and the Reward by enviroment(game)(transition and reward at the same time)\n",
    "            nextJointState , jointReward = self.game.getNextJointState(currentJointState,jointAction)\n",
    "            \n",
    "            #4: with the observed nextJointState and rewardSet, update Q values (iteration through manager)                                                #s              a               s'           r\n",
    "            self.manager.updateQValueSet(currentJointState, jointAction, nextJointState, jointReward)            \n",
    "            \n",
    "            #5: update the next JointState as the currentState\n",
    "            self.manager.updateJointState(nextJointState)            \n",
    "            \n",
    "            #6: check whter game is finished or not\n",
    "            if self.game.goalState[0] == self.manager.currentJointState[0] or self.game.goalState[1] == self.manager.currentJointState[1]:\n",
    "                a = False\n",
    "            \n",
    "            #collect accumulated reward for performance visulization\n",
    "            for n in range(self.agentNum):\n",
    "                accumulatedRewards[n] += (self.game.GAMMA**count)*jointReward[n]   \n",
    "                                \n",
    "            count += 1    \n",
    "            #print(jointAction, jointReward)\n",
    "   \n",
    "        \n",
    "        return accumulatedRewards\n",
    "        \n",
    "    \n",
    "\n",
    "    \n",
    "    #play one episode of game with the current Q values maintained by each agents (evaluation) \n",
    "    def simulateEpisodes(self,number=1):\n",
    "        \n",
    "        #every episode start from game startingState\n",
    "        self.manager.updateJointState(self.game.startState)    \n",
    "        \n",
    "        #accumulated will be computed for every episode\n",
    "        accumulatedRewards = []\n",
    "        for n in range(self.agentNum):\n",
    "            accumulatedRewards.append(0.0)\n",
    "        \n",
    "        #count will be used to discount imediate reward\n",
    "        count = 0\n",
    "        \n",
    "        #if the game cannot find the goal within 30 iterations, we just stop. It means the learning is not yet enough\n",
    "        limit = 30\n",
    "        \n",
    "        a = True\n",
    "        while a:\n",
    "            \n",
    "            #1: get the currentJointState from Manager\n",
    "            currentJointState = self.manager.currentJointState             \n",
    "              \n",
    "            #2: select action at the given state using exploration strategy\n",
    "            jointAction = self.manager.chooseJointAction(currentJointState)\n",
    "\n",
    "            #3: find the nextJointState and the Reward by enviroment(game)(transition and reward at the same time)\n",
    "            nextJointState , jointReward = self.game.getNextJointState(currentJointState,jointAction)\n",
    "            \n",
    "            #4: update the next JointState as the currentState\n",
    "            self.manager.updateJointState(nextJointState)            \n",
    "            \n",
    "            #5: check whter game is finished or not\n",
    "            if self.game.goalState[0] == self.manager.currentJointState[0] or self.game.goalState[1] == self.manager.currentJointState[1]:\n",
    "                a = False\n",
    "            \n",
    "            #finish simulation if iteration reach limit (training is not yet done)\n",
    "            if count == limit:\n",
    "                a = False\n",
    "            \n",
    "            #6: collect accumulated reward for performance visulization\n",
    "            for n in range(self.agentNum):\n",
    "                accumulatedRewards[n] += (self.game.GAMMA**count)*jointReward[n]   \n",
    "            \n",
    "            #increment count for discounting reward \n",
    "            count += 1  \n",
    "            \n",
    "            #print joint action and reward\n",
    "            print(jointAction, jointReward)\n",
    "               \n",
    "        return accumulatedRewards    \n",
    "    \n",
    "    \n",
    "                 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0\n",
      "0 1\n",
      "0 2\n",
      "0 3\n",
      "0 4\n",
      "0 5\n",
      "0 6\n",
      "0 7\n",
      "0 8\n",
      "0 9\n",
      "[1, 0] [-1, -1]\n",
      "[0, 0] [-1, -1]\n",
      "[0, 2] [0, 100]\n",
      "0 10\n",
      "0 11\n",
      "0 12\n",
      "0 13\n",
      "0 14\n",
      "0 15\n",
      "0 16\n",
      "0 17\n",
      "0 18\n",
      "0 19\n",
      "[2, 1] [-1, -1]\n",
      "[2, 1] [-1, -1]\n",
      "[2, 1] [-1, -1]\n",
      "[2, 1] [-1, -1]\n",
      "[2, 1] [-1, -1]\n",
      "[2, 1] [-1, -1]\n",
      "[2, 1] [-1, -1]\n",
      "[2, 1] [-1, -1]\n",
      "[2, 1] [-1, -1]\n",
      "[2, 1] [-1, -1]\n",
      "[2, 1] [-1, -1]\n",
      "[2, 1] [-1, -1]\n",
      "[2, 1] [-1, -1]\n",
      "[2, 1] [-1, -1]\n",
      "[2, 1] [-1, -1]\n",
      "[2, 1] [-1, -1]\n",
      "[2, 1] [-1, -1]\n",
      "[2, 1] [-1, -1]\n",
      "[2, 1] [-1, -1]\n",
      "[2, 1] [-1, -1]\n",
      "[2, 1] [-1, -1]\n",
      "[2, 1] [-1, -1]\n",
      "[2, 1] [-1, -1]\n",
      "[2, 1] [-1, -1]\n",
      "[2, 1] [-1, -1]\n",
      "[2, 1] [-1, -1]\n",
      "[2, 1] [-1, -1]\n",
      "[2, 1] [-1, -1]\n",
      "[2, 1] [-1, -1]\n",
      "[2, 1] [-1, -1]\n",
      "[2, 1] [-1, -1]\n",
      "0 20\n",
      "0 21\n",
      "0 22\n",
      "0 23\n",
      "0 24\n",
      "0 25\n",
      "0 26\n",
      "0 27\n",
      "0 28\n",
      "0 29\n",
      "[0, 2] [-1, -1]\n",
      "[0, 0] [-1, -1]\n",
      "[0, 0] [0, 100]\n",
      "0 30\n",
      "0 31\n",
      "0 32\n",
      "0 33\n",
      "0 34\n",
      "0 35\n",
      "0 36\n",
      "0 37\n",
      "0 38\n",
      "0 39\n",
      "[0, 2] [-1, -1]\n",
      "[0, 0] [-1, -1]\n",
      "[0, 0] [0, 100]\n",
      "0 40\n",
      "0 41\n",
      "0 42\n",
      "0 43\n",
      "0 44\n",
      "0 45\n",
      "0 46\n",
      "0 47\n",
      "0 48\n",
      "0 49\n",
      "[0, 2] [-1, -1]\n",
      "[0, 0] [-1, -1]\n",
      "[0, 0] [0, 100]\n",
      "0 50\n",
      "0 51\n",
      "0 52\n",
      "0 53\n",
      "0 54\n",
      "0 55\n",
      "0 56\n",
      "0 57\n",
      "0 58\n",
      "0 59\n",
      "[0, 2] [-1, -1]\n",
      "[0, 0] [-1, -1]\n",
      "[0, 0] [0, 100]\n",
      "0 60\n",
      "0 61\n",
      "0 62\n",
      "0 63\n",
      "0 64\n",
      "0 65\n",
      "0 66\n",
      "0 67\n",
      "0 68\n",
      "0 69\n",
      "[0, 2] [-1, -1]\n",
      "[0, 0] [-1, -1]\n",
      "[0, 0] [0, 100]\n",
      "0 70\n",
      "0 71\n",
      "0 72\n",
      "0 73\n",
      "0 74\n",
      "0 75\n",
      "0 76\n",
      "0 77\n",
      "0 78\n",
      "0 79\n",
      "[1, 2] [-1, -1]\n",
      "[0, 0] [-1, -1]\n",
      "[0, 0] [0, 100]\n",
      "0 80\n",
      "0 81\n",
      "0 82\n",
      "0 83\n",
      "0 84\n",
      "0 85\n",
      "0 86\n",
      "0 87\n",
      "0 88\n",
      "0 89\n",
      "[0, 2] [-1, -1]\n",
      "[0, 0] [-1, -1]\n",
      "[0, 0] [0, 100]\n",
      "0 90\n",
      "0 91\n",
      "0 92\n",
      "0 93\n",
      "0 94\n",
      "0 95\n",
      "0 96\n",
      "0 97\n",
      "0 98\n",
      "0 99\n",
      "[1, 2] [-1, -1]\n",
      "[0, 0] [-1, -1]\n",
      "[0, 0] [0, 100]\n",
      "0 100\n",
      "0 101\n",
      "0 102\n",
      "0 103\n",
      "0 104\n",
      "0 105\n",
      "0 106\n",
      "0 107\n",
      "0 108\n",
      "0 109\n",
      "[0, 2] [-1, -1]\n",
      "[0, 0] [-1, -1]\n",
      "[0, 0] [0, 100]\n",
      "0 110\n",
      "0 111\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-57-d7fdc45e897e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mepisode\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnumEpisodes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0mprint\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepisode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m         \u001b[0maccumulatedRewards_train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mepisode\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mexp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdoEpisodes\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnumber\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mepisode\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mevaluationPeriod\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-56-d91281e4377f>\u001b[0m in \u001b[0;36mdoEpisodes\u001b[0;34m(self, number)\u001b[0m\n\u001b[1;32m     33\u001b[0m             \u001b[1;31m#2: select action at the given state using exploration strategy\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m             \u001b[1;31m#this is greedy action with Nash equilibrium for exploitation\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m             \u001b[0mjointAction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmanager\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchooseJointAction\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcurrentJointState\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m             \u001b[1;31m#this is random action for exploration\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbinomial\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mEPSILON\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-55-279708fe2d98>\u001b[0m in \u001b[0;36mchooseJointAction\u001b[0;34m(self, currentJointState)\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mjointAction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0magentNum\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m             \u001b[0mjointAction\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0magentSet\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchooseAction\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcurrentJointState\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mjointAction\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-54-ea6bf27358cf>\u001b[0m in \u001b[0;36mchooseAction\u001b[0;34m(self, currentJointState)\u001b[0m\n\u001b[1;32m     90\u001b[0m     \u001b[1;31m#action selection rule: we select the Nash equilibirum action: (a1, a2) = Nash (Q1(s,a1,a2),Q2(s,a1,a2))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mchooseAction\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcurrentJointState\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m         \u001b[0mactions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpayoffs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcomputeNash\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcurrentJointState\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m         \u001b[1;31m#provide target agnet's action only (idx=0 for Agent 1, idx=1 for Agent2)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-54-ea6bf27358cf>\u001b[0m in \u001b[0;36mcomputeNash\u001b[0;34m(self, currentJointState)\u001b[0m\n\u001b[1;32m     49\u001b[0m         \u001b[0meqList\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0mvalueList\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m         \u001b[1;32mfor\u001b[0m \u001b[0meq\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mstageGame\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mequilibria\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m             \u001b[1;31m#eq is represented as follows\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\Jinkyoo\\Anaconda3\\lib\\site-packages\\nash\\game.py\u001b[0m in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0mA\u001b[0m \u001b[0mgenerator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m         \"\"\"\n\u001b[0;32m---> 67\u001b[0;31m         return ((s1, s2)\n\u001b[0m\u001b[1;32m     68\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0ms1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ms2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msup1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msup2\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindifference_strategies\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m                 if self.is_ne((s1, s2), (sup1, sup2)))\n",
      "\u001b[0;32mC:\\Users\\Jinkyoo\\Anaconda3\\lib\\site-packages\\nash\\game.py\u001b[0m in \u001b[0;36mindifference_strategies\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    148\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mpair\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpotential_support_pairs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m             \u001b[0ms1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msolve_indifference\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpayoff_matrices\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpair\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m             \u001b[0ms2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msolve_indifference\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpayoff_matrices\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0mpair\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mobey_support\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpair\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\Jinkyoo\\Anaconda3\\lib\\site-packages\\nash\\game.py\u001b[0m in \u001b[0;36msolve_indifference\u001b[0;34m(A, rows, columns)\u001b[0m\n\u001b[1;32m    178\u001b[0m         \"\"\"\n\u001b[1;32m    179\u001b[0m         \u001b[1;31m# Ensure differences between pairs of pure strategies are the same\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 180\u001b[0;31m         \u001b[0mM\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mA\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrows\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mroll\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mA\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrows\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    181\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m         \u001b[1;31m# Columns that must be played with prob 0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\Jinkyoo\\Anaconda3\\lib\\site-packages\\numpy\\core\\numeric.py\u001b[0m in \u001b[0;36mroll\u001b[0;34m(a, shift, axis)\u001b[0m\n\u001b[1;32m   1396\u001b[0m     \u001b[0mshift\u001b[0m \u001b[1;33m%=\u001b[0m \u001b[0mn\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1397\u001b[0m     \u001b[0mindexes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconcatenate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mshift\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0marange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mshift\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1398\u001b[0;31m     \u001b[0mres\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0ma\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindexes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1399\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mreshape\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1400\u001b[0m         \u001b[0mres\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mres\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#This is excution code:\n",
    "\n",
    "#initialize the game with the size of the grid\n",
    "sg = gridGame2(height=3, width=3)  \n",
    "\n",
    "#initialize the game manager\n",
    "ma = agentManager()\n",
    "\n",
    "#add a specific agent to the manager (the number of game participants = sg.agentNum )\n",
    "for i in range(sg.agentNum): \n",
    "    \n",
    "    #specifiy the agent's learning concept (e.g., qLearner, nashQLearner, minMaxQLearner,...)\n",
    "    agent = nashQLearner(sg,i)\n",
    "    ma.addAgent(agent)\n",
    "\n",
    "#Initialize experimentRunner agent\n",
    "exp = experimentRunner(sg, ma)\n",
    "\n",
    "#specifiy the number of experiment run \n",
    "numRuns=10\n",
    "\n",
    "#specifiy the lenght of episodes\n",
    "numEpisodes=2000\n",
    "\n",
    "#specify the frequency of policy evulation\n",
    "evaluationPeriod =10\n",
    "\n",
    "#Initialize the reward collector\n",
    "accumulatedRewards_train = np.zeros((numRuns,numEpisodes, sg.agentNum))\n",
    "accumulatedRewards_test = np.zeros((numRuns, int(numEpisodes/evaluationPeriod), sg.agentNum))\n",
    "\n",
    "#conduct 'numRuns' of experiments\n",
    "for run in range(0, numRuns):\n",
    "    j=0;\n",
    "    exp.manager.resetQValues()    \n",
    "    accumulatedRewards = np.zeros((sg.agentNum, numEpisodes))\n",
    "    \n",
    "    #in each experiment, conduct 'numEpisodes' of episodes\n",
    "    for episode in range(numEpisodes):\n",
    "        print (run, episode)        \n",
    "        accumulatedRewards_train[run,episode,:]=exp.doEpisodes(number=1)\n",
    "\n",
    "        if (episode+1) % evaluationPeriod == 0:\n",
    "            accumulatedRewards_test[run,j,:]=exp.simulateEpisodes(number=1)\n",
    "            j=j+1 #conter for evaluation number\n",
    "                   \n",
    "avgAccumulatedRewards_train = np.mean(accumulatedRewards_train, axis=0)      \n",
    "avgAccumulatedRewards_test = np.mean(accumulatedRewards_test, axis=0)     \n",
    "\n",
    "\n",
    "#plot results\n",
    "fig = plt.figure(figsize=(20, 10))\n",
    "for n in range(ma.agentNum):\n",
    "    ax = fig.add_subplot(2, 1, n+1)\n",
    "    plt.plot(avgAccumulatedRewards_test[:,n], label='agent '+str(n))\n",
    "    plt.xlabel('episode X '+str(evaluationPeriod))\n",
    "    plt.ylabel('Average accumulated reward per each episode')\n",
    "    plt.legend()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The following problems are coding assignemts. To run the code above, you need to install python module \"nashpy\". Refer the following website to understand how to use it.\n",
    "\n",
    "https://github.com/drvinceknight/Nashpy\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 2\n",
    "## Compete coding \"qLearner\" class by filling up deleted code blocks. Then execute individual Q-learning for gridGame1 and gridGame2. Discuss your results.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 3\n",
    "## Compete coding \"nashQLearner\" class by filling up deleted code blocks. Then execute Nash Q-learning for gridGame1 and gridGame2. Discuss your results. Does it perform better than qLearner?   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
